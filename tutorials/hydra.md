Tutorial: Learning Hydra for configuring ML experiments - Simone Scardapane
=======================================================

9 minut czytania

Ten post jest przeznaczony jako kr贸tkie, samodzielne wprowadzenie do narzdzia Hydra. Om贸wiono w nim wiele temat贸w, w tym jak tworzy obiekty klas, uruchamia przeszukiwanie parametr贸w i sprawdza poprawno konfiguracji w czasie pracy. Wprowadzenie nie obejmuje penego zakresu opcji oferowanych przez Hydr. W tym celu nale偶y zgbi [oryginaln dokumentacj](https://hydra.cc/docs/next/intro).

>  Kod dla tego tutoriala jest dostpny na repozytorium GitHub: [https://github.com/sscardapane/hydra-tutorial](https://github.com/sscardapane/hydra-tutorial).

Instalacja i przegld
-------------------------

Bdziemy korzysta z wersji 1.1 biblioteki Hydra, kt贸r mo偶na zainstalowa poprzez:

    pip install hydra-core==1.1.0.rc1
    

> 锔 Uwaga: kilka poni偶szych instrukcji nie bdzie dziaa poprawnie na poprzednich wersjach biblioteki. Wszystkie zmiany pomidzy wersjami s udokumentowane [na stronie tw贸rc贸w](https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_hydra_main_config_path).

Cz kodu oparta jest na [PyTorch](https://pytorch.org/),ale mo偶na go atwo dostosowa do innych framework贸w gbokiego uczenia. Rozwa偶amy klasyczny scenariusz dostrajania, w kt贸rym dostrajamy may model klasyfikacyjny na wierzchu wstpnie wytrenowanej konwencjonalnej sieci neuronowej. Nasza ostateczna konfiguracja bdzie wyglda tak (rysunek mo偶e z pocztku wydawa si niejasny):

![Alt text](../../../../../../../../../C:/Users/User/Desktop/Matematyka%20stosowana%20-%20II%20stopie%C5%84/III%20semestr/Big%20Data%202022/BigData2022-films/tutorials/hydra.jpg)

Po lewej stronie mamy szereg plik贸w konfiguracyjnych, opisujcych kolejno:

1.  Konfiguracja dla zbioru danych;
2.  Konfiguracja naszej wstpnie wytrenowanej sieci;
3.  Konfiguracja naszego klasyfikatora wytrenowanego "na szczycie" sieci wstpnie wytrenowanej.

W centrum, Hydra automatycznie aduje i komponuje nasze pliki konfiguracyjne, dynamicznie nadpisujc ka偶d warto, kt贸r za偶damy w czasie pracy. 

Po prawej stronie, nasz skrypt treningowy wykorzystuje wynikowy obiekt sownikowy do budowy naszego modelu.

Bdziemy budowa t konfiguracj krok po kroku, poznajc po kolei kilka funkcjonalnoci Hydry. Majc to na uwadze, zaczynajmy!

Pierwszy krok: manipulowanie plikiem YAML
-------------------------------------

Podczas gdy istnieje [ogromna liczba](https://github.com/vinta/awesome-python#configuration) sposob贸w na okrelenie pliku konfiguracyjnego, Hydra pracuje z plikami YAML. Zaczynamy od stworzenia prostego pliku `config.yaml` zawierajcego kilka szczeg贸贸w dotyczcych naszego (faszywego) zbioru danych obrazkowych:

    # config.yaml
    dataset:
      image:
        size: 124
        channels: 3
      classes: 10
    

[OmegaConf](https://omegaconf.readthedocs.io/en/2.0_branch/) jest prost bibliotek umo偶liwiajc dostp i manipulowanie plikami konfiguracyjnymi YAML:

    from omegaconf import OmegaConf
    conf = OmegaConf.load('config.yaml')
    
    # Accessing values (dot notation or dictionary notation)
    print(conf.dataset.image.channels)
    print(conf['dataset']['classes'])
    
    # Modifying a value
    conf.dataset.classes = 15
    

Konfiguracja jest adowana jako obiekt `DictConfig`, kt贸ry zapewnia kilka dodatkowych funkcjonalnoci (zwaszcza zagnie偶d偶anie) w odniesieniu do prymitywnych kontener贸w Pythona. Wicej o *OmegaConf* i `DictConfig` mo偶na przeczyta na stronie z [oficjaln dokumentacj](https://omegaconf.readthedocs.io/en/2.0_branch/usage.html#creating). Wiele zaawansowanych funkcjonalnoci w Hydrze jest zudowana na bazie *OmegaConf*, dziki czemu jest to bardzo przydatna lektura.

adowanie i manipulowanie konfiguracj w skrypcie
------------------------------------------------------

G贸wnym celem Hydry jest zapewnienie prostego sposobu na automatyczne adowanie i manipulowanie plikiem konfiguracyjnym w skrypcie. Aby to zrobi, [dekorujemy](https://www.geeksforgeeks.org/decorators-in-python/) nasz g贸wn funkcj za pomoc `hydra.main`:

    @hydra.main(config_name='config')
    def train(cfg: DictConfig):
        # We simply print the configuration
        print(OmegaConf.to_yaml(cfg))
    
    if __name__=='__main__':
        train()
    

Kiedy wywoujemy nasz skrypt, konfiguracja jest automatycznie adowana:

    python main.py
    # Out: 
    #   dataset:
    #     image:
    #       size: 124
    #       channels: 3
    #   classes: 10
    

Wszystkie parametry mo偶emy modyfikowa "w locie", korzystajc ze [skadni Hydry](https://hydra.cc/docs/next/advanced/override_grammar/basic):

    python main.py dataset.classes=15
    # Out: 
    #   dataset:
    #     image:
    #       size: 124
    #       channels: 3
    #   classes: 15 <-- Changed
    

Skadnia pozwala r贸wnie偶 na [dodawanie lub usuwanie parametr贸w](https://hydra.cc/docs/next/advanced/override_grammar/basic) u偶ywajc odpowiednio `+parameter_to_add` oraz `~parameter_to_remove`.

Manipulowanie rejestratorami i katalogami roboczymi
--------------------------------------------

Domylnie Hydra wykonuje ka偶dy skrypt w innym katalogu, aby unikn nadpisywania wynik贸w z r贸偶nych uruchomie. Domylna nazwa katalogu to `outputs/<day>/<time>/`.

Ka偶dy katalog zawiera output Twojego skryptu, folder `.hydra` zawierajcy pliki konfiguracyjne u偶yte podczas uruchomienia oraz plik `<nazwa>.log` zawierajcy wszystkie dane, kt贸re zostay wysane do rejestratora. W ten spos贸b mo偶emy atwo rejestrowa i przechowywa dodatkowe informacje o naszym uruchomieniu:

    import os, logging
    logger = logging.getLogger(__name__)
    
    @hydra.main(config_path='configs', config_name='config')
    def train(cfg: DictConfig):
    
        # Log the current working directory
        logger.info(f'Working dir: {os.getcwd()}')
    
        # ...
    

Jeli wykonamy skrypt, wszystkie komunikaty dziennika zostan wypisane na ekranie, oraz zapisane w odpowiednim pliku `.log`:

    python main.py
    # Out: [2021-05-20 13:11:44,299][__main__][INFO] - Working dir: c:<...>\outputs\2021-05-20\13-11-44
    

Konfiguracja Hydry jest r贸wnie偶 okrelona w szeregu plik贸w YAML, kt贸re s automatycznie adowane i czone z nasz wasn konfiguracj podczas uruchamiania skryptu, co uatwia dostosowanie zachowania biblioteki. Mo偶emy na przykad [zmieni katalog roboczy dla danego uruchomienia](https://hydra.cc/docs/next/configure_hydra/workdir):

    python main.py hydra.run.dir='outputs/custom_folder'
    # Out: [2021-05-20 13:13:48,057][__main__][INFO] - Working dir: c:<...>\outputs\custom_folder
    

Jeli chcesz nadpisa konfiguracj dla dowolnego uruchomienia skryptu, mo偶esz doda j do pliku konfiguracyjnego. Na przykad, aby [zmodyfikowa wyjcie rejestratora](https://hydra.cc/docs/next/configure_hydra/logging).

adowanie konfiguracji poza skryptem
---------------------------------------------

Czasami mo偶emy potrzebowa zaadowa nasz plik konfiguracyjny poza g贸wn funkcj. Cho mo偶emy to zrobi za pomoc `OmegaConf`, nie zapewnia on wszystkich funkcji dostpnych w Hydrze, w tym wikszoci tego, co wprowadzimy w dalszej czci.

[Compose API](https://hydra.cc/docs/next/advanced/unit_testing) zapewnia alternatywny spos贸b adowania plik贸w konfiguracyjnych. Jest to szczeg贸lnie przydatne wewntrz notatnika *Jupyter Notebooks*, lub jeli chcemy przetestowa jednostkowo pewne funkcje, kt贸re zale偶 od Hydry. Aby zaadowa plik konfiguracyjny, musimy najpierw zainicjalizowa Hydr (wystarczy raz na uruchomienie jdra), a nastpnie zaadowa plik:

    from hydra import initialize, compose
    initialize('.') # Assume the configuration file is in the current folder
    cfg = compose(config_name='config')
    

> 锔 `initialize` mo偶e by wywoane tylko raz. Aby u偶y go lokalnie, mo偶emy zainicjalizowa wewntrz tymczasowego kontekstu:
> 
>     with initialize('.'):
>       # ...
>     
> 
> Jest to niezwykle przydatne w testach jednostkowych. [Compose API](https://hydra.cc/docs/next/advanced/unit_testing) posiada r贸wnie偶 alternatywne wersje do uzyskiwania plik贸w konfiguracyjnych z bezwzgldnych cie偶ek lub modu贸w.

 Finalnie, jeli zainicjowalimy Hydr poza kontekstem, a w dowolnym momencie musimy j ponownie zainicjowa, mo偶emy zresetowa jej wewntrzny stan:

    hydra.core.global_hydra.GlobalHydra.instance().clear()

Parametry mog by r贸wnie偶 nadpisywane podczas komponowania konfiguracji, jak pokazano w przykadzie poni偶ej.    

Tworzenie instacji obiekt贸w
---------------------

Kontunuujc konfiguracj naszego pliku. Nastpnym krokiem jest skonfigurowanie szczeg贸贸w naszej wstpnie wytrenowanej sieci. Poniewa偶 istnieje [du偶a liczba wstpnie wytrenowanych sieci konwolucyjnych](https://pytorch.org/vision/stable/models.html) wewntrz PyTorcha, chcielibymy pozostawi ten wyb贸r jako hiper-parametr.

Na szczcie Hydra ma [prost skadni](https://hydra.cc/docs/next/advanced/instantiate_objects/overview) do tworzenia instancji obiekt贸w lub callables (obiektow, kt贸re zachowuj si jak funkcj). Najpierw dodajemy to do naszego pliku konfiguracyjnego:

    feature_extractor:
      _target_: torchvision.models.alexnet
    

Zwr贸 uwag na specjalny klucz `_target_` okrelajcy klas. Mo偶emy u偶y `hydra.utils.instantiate` do stworzenia obiektu tej klasy jako pierwszego kroku naszego kompletnego modelu:

    import torch
    from hydra.utils import instantiate
    
    net = instantiate(cfg.feature_extractor)
    

Mo偶emy przekaza parametry podczas tworzenia instancji, dodajc dodatkowe klucze do pliku konfiguracyjnego. Na przykad, mo偶emy okreli, czy chcemy zaadowa wstpnie wytrenowane wagi, czy nie (zauwa偶, 偶e `pretrained` musi by poprawnym parametrem dla naszej klasy docelowej):

    feature_extractor:
      _target_: torchvision.models.alexnet
      pretrained: True
    

Nie wymaga to 偶adnych zmian w naszym skrypcie treningowym, poniewa偶 wszystkie parametry zostan automatycznie przekazane podczas tworzenia instancji modelu.

Troch o [programowaniu obiektowym](https://www.kodolamacz.pl/blog/wyzwanie-python-4-programowanie-obiektowe/) w prostym jzyku.

Interpolacja zmiennych
----------------------

Nastpnie musimy skonfigurowa nasz may klasyfikator "na szczycie" wstpnie wytrenowanej sieci. Aby to uproci, mo偶emy skorzysta z kilku przydatnych rzeczy wprowadzonych w najnowszym wydaniu Hydry:

1.  [Instancja rekursywna](https://github.com/facebookresearch/hydra/issues/566) pozwala na rekurencyjne tworzenie instancji dla wz贸w pod warunkiem, 偶e posiadaj one klucz `_target_`.
2.  Mo偶liwe jest okrelenie [argument贸w pozycyjnych podczas tworzenia instancji](https://github.com/facebookresearch/hydra/issues/1432) za pomoc sowa kluczowego `_args_`.

Dziki tym narzdziom, otrzymujemy przykad obiektu `torch.nn.Sequential` z dwoma warstwami:

    _target_: torch.nn.Sequential
    _args_:
      - _target_: torch.nn.Linear
        in_features: 9216
        out_features: 100
    
      - _target_: torch.nn.Linear
        in_features: ${..[0].out_features}
        out_features: ${dataset.classes}
    

R贸wnie偶 wykorzystujemy funkcje OmegaConf zwanej [variable interpolation](https://omegaconf.readthedocs.io/en/2.0_branch/usage.html#variable-interpolation), kt贸ra pozwala nam automatycznie okreli parametry dla naszej ostatniej warstwy poprzez zebranie ich z reszty pliku konfiguracyjnego. Dodatkowo, OmegaConf 2.1 (wydany wraz z Hydr 1.1) wprowadza interpolacje _relative_, dziki czemu mo偶emy zapisa powy偶szy klucz `in_features` jako `in_features: ${.[0].out_features}`.

Mo偶emy zainicjowa wszystkie obiekty tak jak poprzednio:

    print(instantiate(cfg.classifier))
    # Out:
    # Sequential(
    #   (0): Linear(in_features=9216, out_features=100, bias=True)
    #   (1): Linear(in_features=100, out_features=10, bias=True)
    # )
    

Std, mo偶emy uo偶y nasz klasyfikator "na szczycie" wstpnie wytrenowanej sieci:

    classifier = instantiate(cfg.classifier)
    net.classifier = classifier
    

> 锔 Zakadamy, 偶e wstpnie wytrenowana sie ma pole `classifier`. Bardziej elastyczn opcj byoby dodanie tego w samym pliku konfiguracyjnym.

Warto wspomnie, 偶e zmienne s przypisane tylko wtedy, gdy odpowiednie pola s dostpne. Mo偶emy jednak zmodyfikowa nasz skrypt, aby zwraca konfiguracj z wszystkimi zmiennymi, dla atwiejszego debugowania:

    print(OmegaConf.to_yaml(cfg.classifier, resolve=True))
    # Out:
    # _target_: torch.nn.Sequential
    # _args_:
    # - _target_: torch.nn.Linear
    #   in_features: 9216
    #   out_features: 100
    # - _target_: torch.nn.Linear
    #   in_features: 100
    #   out_features: 10
    

Grupy konfiguracyjne
--------------------

W praktyce chcielibymy mie wiele plik贸w konfiguracyjnych, okrelajcych r贸偶ne typy modeli, np. powy偶szy may klasyfikator i wikszy, zo偶ony z wikszej liczby warstw. Mo偶emy to osign za pomoc [grup konfiguracyjnych Hydry](https://hydra.cc/docs/next/tutorials/structured_config/config_groups).

Najpierw grupujemy nasze pliki konfiguracyjne wewntrz folderu `configs`, zawierajcego nasz poprzedni plik `config.yaml`, folder `classifiers` zawierajcy dwa pliki YAML: `small.yaml` (konfiguracja maych klasyfikator贸w) oraz `large.yaml` (konfiguracja du偶ych klasyfikator贸w). Ostateczny katalog roboczy wyglda nastpujco:

    +--main.py
    +--configs/
    |  +--config.yaml --> Main configuration
    |  +--classifiers/
       |  +--small.yaml --> Small classifier
       |  +--large.yaml --> Large classifier
    

Mo偶emy okreli domyln warto dla naszego klasyfikatora u偶ywajc ["default list"](https://hydra.cc/docs/next/advanced/defaults_list/), specjalnego wza `config`, kt贸rego Hydra u偶ywa do budowy kocowego pliku konfiguracyjnego. W praktyce, wprowadzamy do `config.yaml` poni偶sze jako obiekt g贸wny:

    defaults:
      - classifier: small
    

Wewntrz naszego skryptu treningowego klasyfikator bdzie dostpny jako `cfg.classifier`, a jego warto mo偶emy w ka偶dej chwili nadpisa:

    python main.py classifier=large
    

Inicjownie wielu wywoa
-----------------------

Jedn z interesujcych konsekwencji posiadania niezale偶nych folder贸w dla ka偶dego wywoania jest to, 偶e mo偶emy atwo przeprowadzi wiele eksperyment贸w poprzez *sweeping* (zamiatanie) nad pewnymi parametrami, a nastpnie analizowa wyniki patrzc na ka偶dy folder po kolei. Na przykad, mo偶emy uruchomi dwa wywoania z dwoma klasyfikatorami w nastpujcy spos贸b:

    python main.py -m classifier=small,large
    # Out:
    # [2021-05-20 16:10:50,253][HYDRA] Launching 2 jobs locally
    # [2021-05-20 16:10:50,254][HYDRA] 	#0 : classifier=small
    # ...
    # [2021-05-20 16:10:50,370][HYDRA] 	#1 : classifier=large
    # ...
    
Flaga `-m` (alternatywnie, `--multi-run`) instruuje Hydr do wykonania *sweepingu*. Istniej [r贸偶ne sposoby](https://hydra.cc/docs/next/tutorials/basic/running_your_app/multi-run) na okrelenie zakresu *"sweepingu* opr贸cz listy. Hydra obsuguje tak偶e wiele zewntrznych *launcher贸w i sweeper贸w*, kt贸re nie s opisane w tym pocie.

Walidacja konfiguracji za pomoc schematu
------------------------------------------

Zakoczymy przegld Hydry interesujc mo偶liwoci *walidacji* podczas wywoania parametr贸w poprzez okrelenie *schematu konfiguracji*.

>  Schematy mog by r贸wnie偶 u偶ywane jako alternatywa dla plik贸w YAML, ale nie obejmujemy tego u偶ycia tutaj. [Zobacz oficjaln dokumentacj](https://hydra.cc/docs/next/tutorials/structured_config/schema).

Dla uproszczenia pokazujemy tylko jak walidowa parametry dla zbioru danych, a reszt pozostawiamy jako wiczenie. Najpierw definiujemy w naszym kodzie zagnie偶d偶ony zestaw klas `dataclass` naladujcych nasz konfiguracj, oraz deklarujemy typy za pomoc podpowiedzi:

    @dataclass
    class ImageConfig:
        size: int
        channels: int
    
    @dataclass
    class DatasetConfig:
        image: ImageConfig
        classes: int
    
    @dataclass
    class MainConfig:
        dataset : DatasetConfig
        feature_extractor: Any
        classifier: Any
    

[ConfigStore](https://hydra.cc/docs/next/tutorials/structured_config/config_store/) zapewnia pojedynczy interfejs do komunikacji z wntrzem Hydry. W szczeg贸lnoci, mo偶emy go u偶y do przechowywania naszego schematu w nastpujcy spos贸b (zwr贸 uwag na nazw, kt贸r przypisujemy schematowi):

    cs = hydra.core.config_store.ConfigStore()
    cs.store(name="config_schema", node=MainConfig)
    

Wewntrz naszego pliku konfiguracyjnego, mo偶emy powiza schemat poprzez umieszczenie go wewntrz listy domylnej:

    defaults:
       - config_schema
    

Teraz, jeli spr贸bujemy nadpisa parametr niewaciwym typem, wywoanie natychmiast koczy si niepowodzeniem:

    python main.py dataset.classes=0.5
    # Out:
    # Error merging override dataset.classes=0.5
    # Value '0.5' could not be converted to Integer
    #    full_key: dataset.classes
    #    reference_type=DatasetConfig
    #    object_type=DatasetConfig
    
Thats it!

Cay opisany tutaj kod znajduje si na repozytorium GitHub: [https://github.com/sscardapane/hydra-tutorial](https://github.com/sscardapane/hydra-tutorial).

Przekad na jzyk polski wpisu z bloga Simone Scardapane'a [Tutorial: Learning Hydra for configuring ML experiments](https://www.sscardapane.it/tutorials/hydra-tutorial/).