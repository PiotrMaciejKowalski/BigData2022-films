Tutorial: Hydra dla projekt贸w Data Science
=======================================================

Celem tego poradnika jest kr贸tkie wprowadzenie do narzdzia Hydra. Om贸wiono w nim podstawowe operacje bazujc na materiale [Configuration Management For Data Science Made Easy With Hydra](https://www.youtube.com/watch?v=tEsPyYnzt8s).

>  Kod dla tego tutoriala jest dostpny na repozytorium GitHub: [https://github.com/ArjanCodes/2021-config](https://github.com/ArjanCodes/2021-config).

Instalacja i przegld
------------------------------------------------------

Bdziemy korzysta z wersji 1.3 (stable) biblioteki Hydra, kt贸r mo偶na zainstalowa poprzez:
```
pip install hydra-core --upgrade
```

Wprowadzenie
------------------------------------------------------
> "Hydra jest to framework typu open-source, kt贸ry upraszcza rozw贸j bada i innych zo偶onych aplikacji. Kluczow cech jest mo偶liwo dynamicznego tworzenia hierarchicznej konfiguracji przez kompozycj i nadpisywania jej poprzez pliki konfiguracyjne i lini polece. Nazwa Hydra pochodzi od jej zdolnoci do uruchamiania wielu podobnych zada - bardzo podobnie jak Hydra z wieloma gowami." [hydra.cc/docs/intro/](https://hydra.cc/docs/intro/)

**<font size = "5"><center>Przykad struktury Hydry dla modelu CNN</center></font>**

![Alt text](hydra_files/hydra.jpg)

Po lewej stronie mamy szereg plik贸w konfiguracyjnych, opisujcych kolejno:

1.  Konfiguracja dla zbioru danych;
2.  Konfiguracja naszej wstpnie wytrenowanej sieci;
3.  Konfiguracja naszego klasyfikatora wytrenowanego "na szczycie" sieci wstpnie wytrenowanej.

W centrum, Hydra automatycznie aduje i komponuje nasze pliki konfiguracyjne, dynamicznie nadpisujc ka偶d warto, kt贸r za偶damy w czasie pracy. 

Po prawej stronie, nasz skrypt treningowy wykorzystuje wynikowy obiekt sownikowy do budowy naszego modelu.

> 锔 Uwaga: Bdziemy usprawnia gotowy projekt dla modelu [LinearNet()](https://github.com/ArjanCodes/2021-config/tree/main/before).

main.py
------------------------------------------------------
Plik `main.py` bdzie plikiem, kt贸ry z wykrzystaniem torch'a oraz modu贸w z wewntrz projektu:
- buduje model klasyfikacyjny typu LinearNet(),
- aduje dane (zbi贸r MNIST),
- przeprowadza wielokrotne uczenie modelu,
- wywietla urednione metryki.
```
# import zewntrznych bibliotek
import pathlib
import torch

# import modu贸w z wewntrz projektu
from ds.dataset import create_dataloader
from ds.models import LinearNet
from ds.runner import Runner, run_epoch
from ds.tracking import TensorboardExperiment

# Hiperparametry dla modelu
EPOCH_COUNT = 20
LR = 5e-5
BATCH_SIZE = 128
LOG_PATH = "./runs"

# cie偶ki do danych
DATA_DIR = "../data/raw"
TEST_DATA = pathlib.Path(f"{DATA_DIR}/t10k-images-idx3-ubyte.gz")
TEST_LABELS = pathlib.Path(f"{DATA_DIR}/t10k-labels-idx1-ubyte.gz")
TRAIN_DATA = pathlib.Path(f"{DATA_DIR}/train-images-idx3-ubyte.gz")
TRAIN_LABELS = pathlib.Path(f"{DATA_DIR}/train-labels-idx1-ubyte.gz")

# aplikacja uruchamiajca ca procedur modelu (zaadowanie danych -> uczenie modelu -> wywietlenie metryk)
def main():

    # Model + optymalizator
    model = LinearNet()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    # customowa funkcja adowania danych MNIST
    test_loader = create_dataloader(BATCH_SIZE, TEST_DATA, TEST_LABELS)
    train_loader = create_dataloader(BATCH_SIZE, TRAIN_DATA, TRAIN_LABELS)

    # customowa funkcja "dopasowujca" model do danych
    test_runner = Runner(test_loader, model)
    train_runner = Runner(train_loader, model, optimizer)

    # customowa funkcja ledzenia wynik贸w modelu
    tracker = TensorboardExperiment(log_path=LOG_PATH)

    # iteracyjne uczenie modelu (ze wzgldu na zadany parametr EPOCH_COUNT)
    for epoch_id in range(EPOCH_COUNT):
        # customowa funkcja uruchamiajca uczenie modelu
        run_epoch(test_runner, train_runner, tracker, epoch_id)

        # wyliczanie urednionych metryk z wykonanych epok
        summary = ", ".join(
            [
                f"[Epoch: {epoch_id + 1}/{EPOCH_COUNT}]",
                f"Test Accuracy: {test_runner.avg_accuracy: 0.4f}",
                f"Train Accuracy: {train_runner.avg_accuracy: 0.4f}",
            ]
        )
        print("\n" + summary + "\n")

        # Reset the runners
        train_runner.reset()
        test_runner.reset()

        # Flush the tracker after every epoch for live updates
        tracker.flush()


if __name__ == "__main__":
    main()
```

Pierwsze co mo偶emy dostrzec, jest zbdno plik贸w konfiguracyjnych. W celu wyczyszczenia `main.py`, przeniesiemy nasze hiperparametry oraz cie偶ki do oddzielnego pliku typu `.yaml`.

config.yaml
------------------------------------------------------
Najlepsz praktyk jest utworzenie dodatkowego folderu konfiguracyjnego w projekcie, przechowujcego wszystkie pliki konfiguracyjne. Plik konfiguracyjny `config.yaml` dla naszego projektu bdzie wyglda nastpujco:
```
# config.yaml
params:
    epoch_count = 20
    lr = 5e-5
    batch_size = 128
``` 
Tym sposobem, utworzylimy plik konfiguracyjny z grup o nazwie `params` oraz z zadanymi dla tej grupy trzema wartociami.
> Nale偶y pamita, aby wszelkie wartoci w pliku konfiguracyjnym zapisywa **maymi literami!**

Jak to wpynie na `main.py`?
```
# import zewntrznych bibliotek
import pathlib
import torch

import hydra # IMPORTOWANIE BIBLIOTEKI HYDRA

# import modu贸w z wewntrz projektu
from ds.dataset import create_dataloader
from ds.models import LinearNet
from ds.runner import Runner, run_epoch
from ds.tracking import TensorboardExperiment

# Hiperparametry dla modelu
DATA_DIR = "../data/raw"

# cie偶ki do danych
TEST_DATA = pathlib.Path(f"{DATA_DIR}/t10k-images-idx3-ubyte.gz")
TEST_LABELS = pathlib.Path(f"{DATA_DIR}/t10k-labels-idx1-ubyte.gz")
TRAIN_DATA = pathlib.Path(f"{DATA_DIR}/train-images-idx3-ubyte.gz")
TRAIN_LABELS = pathlib.Path(f"{DATA_DIR}/train-labels-idx1-ubyte.gz")

# aplikacja uruchamiajca ca procedur modelu (zaadowanie danych -> uczenie modelu -> wywietlenie metryk)
@hydra.main(config_path="conf", config_name="config") # WYKORZYSTANIE DEKORATORA, ABY WSKAZA MIEJSCE POO呕ENIA PLIKU KONFIGURACYJNEGO (config_path) ORAZ JEGO NAZWY (config_name)
def main(cfg):

    # Model + optymalizator
    model = LinearNet()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    # customowa funkcja adowania danych MNIST
    test_loader = create_dataloader(BATCH_SIZE, TEST_DATA, TEST_LABELS)
    train_loader = create_dataloader(BATCH_SIZE, TRAIN_DATA, TRAIN_LABELS)

    # customowa funkcja "dopasowujca" model do danych
    test_runner = Runner(test_loader, model)
    train_runner = Runner(train_loader, model, optimizer)

    # customowa funkcja ledzenia wynik贸w modelu
    tracker = TensorboardExperiment(log_path=LOG_PATH)

    # iteracyjne uczenie modelu (ze wzgldu na zadany parametr EPOCH_COUNT)
    for epoch_id in range(EPOCH_COUNT):
        # customowa funkcja uruchamiajca uczenie modelu
        run_epoch(test_runner, train_runner, tracker, epoch_id)

        # wyliczanie urednionych metryk z wykonanych epok
        summary = ", ".join(
            [
                f"[Epoch: {epoch_id + 1}/{EPOCH_COUNT}]",
                f"Test Accuracy: {test_runner.avg_accuracy: 0.4f}",
                f"Train Accuracy: {train_runner.avg_accuracy: 0.4f}",
            ]
        )
        print("\n" + summary + "\n")

        # Reset the runners
        train_runner.reset()
        test_runner.reset()

        # Flush the tracker after every epoch for live updates
        tracker.flush()


if __name__ == "__main__":
    main()
```
Jak widzimy, linijki zawierajce hiperparametry zoztay przeniesione do pliku konfiguracyjnego. Dodatkowo, pojawi si parametr `cfg`, kt贸ry oznacza, 偶e Hydra automatycznie wprowadzi parametry z pliku `config.yaml` do funkcji `main()`.

Przeniemy r贸wnie偶 cie偶ki do pliku konfiguracyjnego. W tym celu dodamy kolejn grup `files`.

```
# config.yaml
files:
    test_data: t10k-images-idx3-ubyte.gz
    test_labels: t10k-labels-idx1-ubyte.gz
    train_data: train-images-idx3-ubyte.gz
    train_labels: train-labels-idx1-ubyte.gz
params:
    epoch_count = 20
    lr = 5e-5
    batch_size = 128
``` 

https://youtu.be/tEsPyYnzt8s?t=715



























Podczas gdy istnieje [ogromna liczba](https://github.com/vinta/awesome-python#configuration) sposob贸w na okrelenie pliku konfiguracyjnego, Hydra pracuje z plikami YAML. Zaczynamy od stworzenia prostego pliku `config.yaml` zawierajcego kilka szczeg贸贸w dotyczcych naszego (faszywego) zbioru danych obrazkowych:


[OmegaConf](https://omegaconf.readthedocs.io/en/2.0_branch/) jest prost bibliotek umo偶liwiajc dostp i manipulowanie plikami konfiguracyjnymi YAML:
```
from omegaconf import OmegaConf
conf = OmegaConf.load('config.yaml')

# Accessing values (dot notation or dictionary notation)
print(conf.dataset.image.channels)
print(conf['dataset']['classes'])

# Modifying a value
conf.dataset.classes = 15
```

Konfiguracja jest adowana jako obiekt `DictConfig`, kt贸ry zapewnia kilka dodatkowych funkcjonalnoci (zwaszcza zagnie偶d偶anie) w odniesieniu do prymitywnych kontener贸w Pythona. Wicej o *OmegaConf* i `DictConfig` mo偶na przeczyta na stronie z [oficjaln dokumentacj](https://omegaconf.readthedocs.io/en/2.0_branch/usage.html#creating). Wiele zaawansowanych funkcjonalnoci w Hydrze jest zudowana na bazie *OmegaConf*, dziki czemu jest to bardzo przydatna lektura.

adowanie i manipulowanie konfiguracj w skrypcie
------------------------------------------------------

G贸wnym celem Hydry jest zapewnienie prostego sposobu na automatyczne adowanie i manipulowanie plikiem konfiguracyjnym w skrypcie. Aby to zrobi, [dekorujemy](https://www.geeksforgeeks.org/decorators-in-python/) nasz g贸wn funkcj za pomoc `hydra.main`:
```
@hydra.main(config_name='config')
def train(cfg: DictConfig):
    # We simply print the configuration
    print(OmegaConf.to_yaml(cfg))

if __name__=='__main__':
    train()
``` 

Kiedy wywoujemy nasz skrypt, konfiguracja jest automatycznie adowana:
```
python main.py
# Out: 
#   dataset:
#     image:
#       size: 124
#       channels: 3
#   classes: 10
```

Wszystkie parametry mo偶emy modyfikowa "w locie", korzystajc ze [skadni Hydry](https://hydra.cc/docs/next/advanced/override_grammar/basic):
```
python main.py dataset.classes=15
# Out: 
#   dataset:
#     image:
#       size: 124
#       channels: 3
#   classes: 15 <-- Changed
```

Skadnia pozwala r贸wnie偶 na [dodawanie lub usuwanie parametr贸w](https://hydra.cc/docs/next/advanced/override_grammar/basic) u偶ywajc odpowiednio `+parameter_to_add` oraz `~parameter_to_remove`.

Manipulowanie rejestratorami i katalogami roboczymi
------------------------------------------------------

Domylnie Hydra wykonuje ka偶dy skrypt w innym katalogu, aby unikn nadpisywania wynik贸w z r贸偶nych uruchomie. Domylna nazwa katalogu to `outputs/<day>/<time>/`.

Ka偶dy katalog zawiera output Twojego skryptu, folder `.hydra` zawierajcy pliki konfiguracyjne u偶yte podczas uruchomienia oraz plik `<nazwa>.log` zawierajcy wszystkie dane, kt贸re zostay wysane do rejestratora. W ten spos贸b mo偶emy atwo rejestrowa i przechowywa dodatkowe informacje o naszym uruchomieniu:
```
import os, logging
logger = logging.getLogger(__name__)

@hydra.main(config_path='configs', config_name='config')
def train(cfg: DictConfig):

    # Log the current working directory
    logger.info(f'Working dir: {os.getcwd()}')

    # ...
```

Jeli wykonamy skrypt, wszystkie komunikaty dziennika zostan wypisane na ekranie, oraz zapisane w odpowiednim pliku `.log`:
```
python main.py
# Out: [2021-05-20 13:11:44,299][__main__][INFO] - Working dir: c:<...>\outputs\2021-05-20\13-11-44
```

Konfiguracja Hydry jest r贸wnie偶 okrelona w szeregu plik贸w YAML, kt贸re s automatycznie adowane i czone z nasz wasn konfiguracj podczas uruchamiania skryptu, co uatwia dostosowanie zachowania biblioteki. Mo偶emy na przykad [zmieni katalog roboczy dla danego uruchomienia](https://hydra.cc/docs/next/configure_hydra/workdir):
```
python main.py hydra.run.dir='outputs/custom_folder'
# Out: [2021-05-20 13:13:48,057][__main__][INFO] - Working dir: c:<...>\outputs\custom_folder
```

Jeli chcesz nadpisa konfiguracj dla dowolnego uruchomienia skryptu, mo偶esz doda j do pliku konfiguracyjnego. Na przykad, aby [zmodyfikowa wyjcie rejestratora](https://hydra.cc/docs/next/configure_hydra/logging).

adowanie konfiguracji poza skryptem
------------------------------------------------------

Czasami mo偶emy potrzebowa zaadowa nasz plik konfiguracyjny poza g贸wn funkcj. Cho mo偶emy to zrobi za pomoc `OmegaConf`, nie zapewnia on wszystkich funkcji dostpnych w Hydrze, w tym wikszoci tego, co wprowadzimy w dalszej czci.

[Compose API](https://hydra.cc/docs/next/advanced/unit_testing) zapewnia alternatywny spos贸b adowania plik贸w konfiguracyjnych. Jest to szczeg贸lnie przydatne wewntrz notatnika *Jupyter Notebooks*, lub jeli chcemy przetestowa jednostkowo pewne funkcje, kt贸re zale偶 od Hydry. Aby zaadowa plik konfiguracyjny, musimy najpierw zainicjalizowa Hydr (wystarczy raz na uruchomienie jdra), a nastpnie zaadowa plik:
```
from hydra import initialize, compose
initialize('.') # Assume the configuration file is in the current folder
cfg = compose(config_name='config')
```

> 锔 `initialize` mo偶e by wywoane tylko raz. Aby u偶y go lokalnie, mo偶emy zainicjalizowa wewntrz tymczasowego kontekstu:
> ```
>with initialize('.'):
>   # ...
>```
> 
> Jest to niezwykle przydatne w testach jednostkowych. [Compose API](https://hydra.cc/docs/next/advanced/unit_testing) posiada r贸wnie偶 alternatywne wersje do uzyskiwania plik贸w konfiguracyjnych z bezwzgldnych cie偶ek lub modu贸w.

 Finalnie, jeli zainicjowalimy Hydr poza kontekstem, a w dowolnym momencie musimy j ponownie zainicjowa, mo偶emy zresetowa jej wewntrzny stan:
```
hydra.core.global_hydra.GlobalHydra.instance().clear()
```
Parametry mog by r贸wnie偶 nadpisywane podczas komponowania konfiguracji, jak pokazano w przykadzie poni偶ej.    

Tworzenie instacji obiekt贸w
------------------------------------------------------

Kontunuujc konfiguracj naszego pliku. Nastpnym krokiem jest skonfigurowanie szczeg贸贸w naszej wstpnie wytrenowanej sieci. Poniewa偶 istnieje [du偶a liczba wstpnie wytrenowanych sieci konwolucyjnych](https://pytorch.org/vision/stable/models.html) wewntrz PyTorcha, chcielibymy pozostawi ten wyb贸r jako hiper-parametr.

Na szczcie Hydra ma [prost skadni](https://hydra.cc/docs/next/advanced/instantiate_objects/overview) do tworzenia instancji obiekt贸w lub callables (obiektow, kt贸re zachowuj si jak funkcj). Najpierw dodajemy to do naszego pliku konfiguracyjnego:
```
feature_extractor:
    _target_: torchvision.models.alexnet
``` 

Zwr贸 uwag na specjalny klucz `_target_` okrelajcy klas. Mo偶emy u偶y `hydra.utils.instantiate` do stworzenia obiektu tej klasy jako pierwszego kroku naszego kompletnego modelu:
```
import torch
from hydra.utils import instantiate

net = instantiate(cfg.feature_extractor)
```

Mo偶emy przekaza parametry podczas tworzenia instancji, dodajc dodatkowe klucze do pliku konfiguracyjnego. Na przykad, mo偶emy okreli, czy chcemy zaadowa wstpnie wytrenowane wagi, czy nie (zauwa偶, 偶e `pretrained` musi by poprawnym parametrem dla naszej klasy docelowej):
```
feature_extractor:
    _target_: torchvision.models.alexnet
    pretrained: True
```

Nie wymaga to 偶adnych zmian w naszym skrypcie treningowym, poniewa偶 wszystkie parametry zostan automatycznie przekazane podczas tworzenia instancji modelu.

Troch o [programowaniu obiektowym](https://www.kodolamacz.pl/blog/wyzwanie-python-4-programowanie-obiektowe/) w prostym jzyku.

Interpolacja zmiennych
------------------------------------------------------

Nastpnie musimy skonfigurowa nasz may klasyfikator "na szczycie" wstpnie wytrenowanej sieci. Aby to uproci, mo偶emy skorzysta z kilku przydatnych rzeczy wprowadzonych w najnowszym wydaniu Hydry:

1.  [Instancja rekursywna](https://github.com/facebookresearch/hydra/issues/566) pozwala na rekurencyjne tworzenie instancji dla wz贸w pod warunkiem, 偶e posiadaj one klucz `_target_`.
2.  Mo偶liwe jest okrelenie [argument贸w pozycyjnych podczas tworzenia instancji](https://github.com/facebookresearch/hydra/issues/1432) za pomoc sowa kluczowego `_args_`.

Dziki tym narzdziom, otrzymujemy przykad obiektu `torch.nn.Sequential` z dwoma warstwami:
```
_target_: torch.nn.Sequential
_args_:
    - _target_: torch.nn.Linear
    in_features: 9216
    out_features: 100

    - _target_: torch.nn.Linear
    in_features: ${..[0].out_features}
    out_features: ${dataset.classes}
```

R贸wnie偶 wykorzystujemy funkcje OmegaConf zwanej [variable interpolation](https://omegaconf.readthedocs.io/en/2.0_branch/usage.html#variable-interpolation), kt贸ra pozwala nam automatycznie okreli parametry dla naszej ostatniej warstwy poprzez zebranie ich z reszty pliku konfiguracyjnego. Dodatkowo, OmegaConf 2.1 (wydany wraz z Hydr 1.1) wprowadza interpolacje _relative_, dziki czemu mo偶emy zapisa powy偶szy klucz `in_features` jako `in_features: ${.[0].out_features}`.

Mo偶emy zainicjowa wszystkie obiekty tak jak poprzednio:
```
print(instantiate(cfg.classifier))
# Out:
# Sequential(
#   (0): Linear(in_features=9216, out_features=100, bias=True)
#   (1): Linear(in_features=100, out_features=10, bias=True)
# )
```

Std, mo偶emy uo偶y nasz klasyfikator "na szczycie" wstpnie wytrenowanej sieci:
```
classifier = instantiate(cfg.classifier)
net.classifier = classifier
```

> 锔 Zakadamy, 偶e wstpnie wytrenowana sie ma pole `classifier`. Bardziej elastyczn opcj byoby dodanie tego w samym pliku konfiguracyjnym.

Warto wspomnie, 偶e zmienne s przypisane tylko wtedy, gdy odpowiednie pola s dostpne. Mo偶emy jednak zmodyfikowa nasz skrypt, aby zwraca konfiguracj z wszystkimi zmiennymi, dla atwiejszego debugowania:
```
print(OmegaConf.to_yaml(cfg.classifier, resolve=True))
# Out:
# _target_: torch.nn.Sequential
# _args_:
# - _target_: torch.nn.Linear
#   in_features: 9216
#   out_features: 100
# - _target_: torch.nn.Linear
#   in_features: 100
#   out_features: 10
```

Grupy konfiguracyjne
------------------------------------------------------

W praktyce chcielibymy mie wiele plik贸w konfiguracyjnych, okrelajcych r贸偶ne typy modeli, np. powy偶szy may klasyfikator i wikszy, zo偶ony z wikszej liczby warstw. Mo偶emy to osign za pomoc [grup konfiguracyjnych Hydry](https://hydra.cc/docs/next/tutorials/structured_config/config_groups).

Najpierw grupujemy nasze pliki konfiguracyjne wewntrz folderu `configs`, zawierajcego nasz poprzedni plik `config.yaml`, folder `classifiers` zawierajcy dwa pliki YAML: `small.yaml` (konfiguracja maych klasyfikator贸w) oraz `large.yaml` (konfiguracja du偶ych klasyfikator贸w). Ostateczny katalog roboczy wyglda nastpujco:
```
+--main.py
+--configs/
|  +--config.yaml --> Main configuration
|  +--classifiers/
    |  +--small.yaml --> Small classifier
    |  +--large.yaml --> Large classifier
```

Mo偶emy okreli domyln warto dla naszego klasyfikatora u偶ywajc ["default list"](https://hydra.cc/docs/next/advanced/defaults_list/), specjalnego wza `config`, kt贸rego Hydra u偶ywa do budowy kocowego pliku konfiguracyjnego. W praktyce, wprowadzamy do `config.yaml` poni偶sze jako obiekt g贸wny:
```
defaults:
    - classifier: small
```

Wewntrz naszego skryptu treningowego klasyfikator bdzie dostpny jako `cfg.classifier`, a jego warto mo偶emy w ka偶dej chwili nadpisa:
```
python main.py classifier=large
```

Inicjownie wielu wywoa
------------------------------------------------------

Jedn z interesujcych konsekwencji posiadania niezale偶nych folder贸w dla ka偶dego wywoania jest to, 偶e mo偶emy atwo przeprowadzi wiele eksperyment贸w poprzez *sweeping* (zamiatanie) nad pewnymi parametrami, a nastpnie analizowa wyniki patrzc na ka偶dy folder po kolei. Na przykad, mo偶emy uruchomi dwa wywoania z dwoma klasyfikatorami w nastpujcy spos贸b:
```
python main.py -m classifier=small,large
# Out:
# [2021-05-20 16:10:50,253][HYDRA] Launching 2 jobs locally
# [2021-05-20 16:10:50,254][HYDRA] 	#0 : classifier=small
# ...
# [2021-05-20 16:10:50,370][HYDRA] 	#1 : classifier=large
# ...
```

Flaga `-m` (alternatywnie, `--multi-run`) instruuje Hydr do wykonania *sweepingu*. Istniej [r贸偶ne sposoby](https://hydra.cc/docs/next/tutorials/basic/running_your_app/multi-run) na okrelenie zakresu *"sweepingu* opr贸cz listy. Hydra obsuguje tak偶e wiele zewntrznych *launcher贸w i sweeper贸w*, kt贸re nie s opisane w tym pocie.

Walidacja konfiguracji za pomoc schematu
------------------------------------------------------

Zakoczymy przegld Hydry interesujc mo偶liwoci *walidacji* podczas wywoania parametr贸w poprzez okrelenie *schematu konfiguracji*.

>  Schematy mog by r贸wnie偶 u偶ywane jako alternatywa dla plik贸w YAML, ale nie obejmujemy tego u偶ycia tutaj. [Zobacz oficjaln dokumentacj](https://hydra.cc/docs/next/tutorials/structured_config/schema).

Dla uproszczenia pokazujemy tylko jak walidowa parametry dla zbioru danych, a reszt pozostawiamy jako wiczenie. Najpierw definiujemy w naszym kodzie zagnie偶d偶ony zestaw klas `dataclass` naladujcych nasz konfiguracj, oraz deklarujemy typy za pomoc podpowiedzi:
```
@dataclass
class ImageConfig:
    size: int
    channels: int

@dataclass
class DatasetConfig:
    image: ImageConfig
    classes: int

@dataclass
class MainConfig:
    dataset : DatasetConfig
    feature_extractor: Any
    classifier: Any
```

[ConfigStore](https://hydra.cc/docs/next/tutorials/structured_config/config_store/) zapewnia pojedynczy interfejs do komunikacji z wntrzem Hydry. W szczeg贸lnoci, mo偶emy go u偶y do przechowywania naszego schematu w nastpujcy spos贸b (zwr贸 uwag na nazw, kt贸r przypisujemy schematowi):
```
cs = hydra.core.config_store.ConfigStore()
cs.store(name="config_schema", node=MainConfig)
```

Wewntrz naszego pliku konfiguracyjnego, mo偶emy powiza schemat poprzez umieszczenie go wewntrz listy domylnej:
```
defaults:
    - config_schema
```

Teraz, jeli spr贸bujemy nadpisa parametr niewaciwym typem, wywoanie natychmiast koczy si niepowodzeniem:
```
python main.py dataset.classes=0.5
# Out:
# Error merging override dataset.classes=0.5
# Value '0.5' could not be converted to Integer
#    full_key: dataset.classes
#    reference_type=DatasetConfig
#    object_type=DatasetConfig
```
Thats it!

Cay opisany tutaj kod znajduje si na repozytorium GitHub: [https://github.com/sscardapane/hydra-tutorial](https://github.com/sscardapane/hydra-tutorial).

Przekad na jzyk polski wpisu z bloga Simone Scardapane'a [Tutorial: Learning Hydra for configuring ML experiments](https://www.sscardapane.it/tutorials/hydra-tutorial/).