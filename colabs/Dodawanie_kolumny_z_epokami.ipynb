{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrMaciejKowalski/BigData2022-films/blob/Dodawanie_kolumny_z_epokami/colabs/Dodawanie_kolumny_z_epokami.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkw2x1zr6CTx"
      },
      "source": [
        "# Wczytanie danych"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark2.4.5\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "# unzip it\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "# install findspark\n",
        "!pip install -q findspark\n",
        "# clone github repo\n",
        "!git clone https://github.com/PiotrMaciejKowalski/BigData2022-films\n",
        "# Przeniesienie plików z BigData2022-films do katalogu nadrzędnego\n",
        "!mv BigData2022-films/* .\n",
        "!mv BigData2022-films/.* .\n",
        "!rmdir BigData2022-films\n",
        "     \n",
        "import os\n",
        "\n",
        "# setup environment variables for our Spark Session to work\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.2.1-bin-hadoop3.2'\n",
        "\n",
        "from lib.pyspark_startup import init, load\n",
        "\n",
        "spark = init()\n",
        "     \n",
        "!chmod +x data_source.sh\n",
        "!./data_source.sh\n",
        "\n",
        "df = load(spark)\n",
        "!git checkout -f Dodawanie_kolumny_z_epokami"
      ],
      "metadata": {
        "id": "zVpAgnBFGtIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3dbca9-945b-48c8-de79-3feefbd6be4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BigData2022-films'...\n",
            "remote: Enumerating objects: 593, done.\u001b[K\n",
            "remote: Counting objects: 100% (208/208), done.\u001b[K\n",
            "remote: Compressing objects: 100% (136/136), done.\u001b[K\n",
            "remote: Total 593 (delta 132), reused 79 (delta 71), pack-reused 385\u001b[K\n",
            "Receiving objects: 100% (593/593), 2.86 MiB | 9.25 MiB/s, done.\n",
            "Resolving deltas: 100% (308/308), done.\n",
            "mv: cannot move 'BigData2022-films/.' to './.': Device or resource busy\n",
            "mv: cannot move 'BigData2022-films/..' to './..': Device or resource busy\n",
            "--2022-12-16 18:26:52--  https://datasets.imdbws.com/name.basics.tsv.gz\n",
            "Resolving datasets.imdbws.com (datasets.imdbws.com)... 13.35.24.124, 13.35.24.56, 13.35.24.122, ...\n",
            "Connecting to datasets.imdbws.com (datasets.imdbws.com)|13.35.24.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 237981379 (227M) [binary/octet-stream]\n",
            "Saving to: ‘name.basics.tsv.gz’\n",
            "\n",
            "name.basics.tsv.gz  100%[===================>] 226.96M  21.9MB/s    in 12s     \n",
            "\n",
            "2022-12-16 18:27:05 (19.3 MB/s) - ‘name.basics.tsv.gz’ saved [237981379/237981379]\n",
            "\n",
            "--2022-12-16 18:27:05--  https://datasets.imdbws.com/title.akas.tsv.gz\n",
            "Resolving datasets.imdbws.com (datasets.imdbws.com)... 13.35.24.124, 13.35.24.56, 13.35.24.122, ...\n",
            "Connecting to datasets.imdbws.com (datasets.imdbws.com)|13.35.24.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 290881394 (277M) [binary/octet-stream]\n",
            "Saving to: ‘title.akas.tsv.gz’\n",
            "\n",
            "title.akas.tsv.gz   100%[===================>] 277.41M  21.7MB/s    in 14s     \n",
            "\n",
            "2022-12-16 18:27:20 (19.2 MB/s) - ‘title.akas.tsv.gz’ saved [290881394/290881394]\n",
            "\n",
            "--2022-12-16 18:27:20--  https://datasets.imdbws.com/title.basics.tsv.gz\n",
            "Resolving datasets.imdbws.com (datasets.imdbws.com)... 13.35.24.124, 13.35.24.56, 13.35.24.122, ...\n",
            "Connecting to datasets.imdbws.com (datasets.imdbws.com)|13.35.24.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165217058 (158M) [binary/octet-stream]\n",
            "Saving to: ‘title.basics.tsv.gz’\n",
            "\n",
            "title.basics.tsv.gz 100%[===================>] 157.56M  22.4MB/s    in 8.4s    \n",
            "\n",
            "2022-12-16 18:27:29 (18.8 MB/s) - ‘title.basics.tsv.gz’ saved [165217058/165217058]\n",
            "\n",
            "--2022-12-16 18:27:29--  https://datasets.imdbws.com/title.crew.tsv.gz\n",
            "Resolving datasets.imdbws.com (datasets.imdbws.com)... 13.35.24.124, 13.35.24.122, 13.35.24.74, ...\n",
            "Connecting to datasets.imdbws.com (datasets.imdbws.com)|13.35.24.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63385834 (60M) [binary/octet-stream]\n",
            "Saving to: ‘title.crew.tsv.gz’\n",
            "\n",
            "title.crew.tsv.gz   100%[===================>]  60.45M  17.2MB/s    in 4.0s    \n",
            "\n",
            "2022-12-16 18:27:34 (15.0 MB/s) - ‘title.crew.tsv.gz’ saved [63385834/63385834]\n",
            "\n",
            "--2022-12-16 18:27:34--  https://datasets.imdbws.com/title.episode.tsv.gz\n",
            "Resolving datasets.imdbws.com (datasets.imdbws.com)... 13.35.24.124, 13.35.24.122, 13.35.24.74, ...\n",
            "Connecting to datasets.imdbws.com (datasets.imdbws.com)|13.35.24.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38913271 (37M) [binary/octet-stream]\n",
            "Saving to: ‘title.episode.tsv.gz’\n",
            "\n",
            "title.episode.tsv.g 100%[===================>]  37.11M  12.5MB/s    in 3.0s    \n",
            "\n",
            "2022-12-16 18:27:38 (12.5 MB/s) - ‘title.episode.tsv.gz’ saved [38913271/38913271]\n",
            "\n",
            "--2022-12-16 18:27:38--  https://datasets.imdbws.com/title.principals.tsv.gz\n",
            "Resolving datasets.imdbws.com (datasets.imdbws.com)... 13.35.24.124, 13.35.24.122, 13.35.24.74, ...\n",
            "Connecting to datasets.imdbws.com (datasets.imdbws.com)|13.35.24.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 420907948 (401M) [binary/octet-stream]\n",
            "Saving to: ‘title.principals.tsv.gz’\n",
            "\n",
            "title.principals.ts 100%[===================>] 401.41M  21.9MB/s    in 21s     \n",
            "\n",
            "2022-12-16 18:28:00 (19.0 MB/s) - ‘title.principals.tsv.gz’ saved [420907948/420907948]\n",
            "\n",
            "--2022-12-16 18:28:00--  https://datasets.imdbws.com/title.ratings.tsv.gz\n",
            "Resolving datasets.imdbws.com (datasets.imdbws.com)... 13.35.7.62, 13.35.7.26, 13.35.7.116, ...\n",
            "Connecting to datasets.imdbws.com (datasets.imdbws.com)|13.35.7.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6315237 (6.0M) [binary/octet-stream]\n",
            "Saving to: ‘title.ratings.tsv.gz’\n",
            "\n",
            "title.ratings.tsv.g 100%[===================>]   6.02M  4.37MB/s    in 1.4s    \n",
            "\n",
            "2022-12-16 18:28:02 (4.37 MB/s) - ‘title.ratings.tsv.gz’ saved [6315237/6315237]\n",
            "\n",
            "Branch 'Dodawanie_kolumny_z_epokami' set up to track remote branch 'Dodawanie_kolumny_z_epokami' from 'origin'.\n",
            "Switched to a new branch 'Dodawanie_kolumny_z_epokami'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szQRCHGBH6qD",
        "outputId": "c1788016-0e98-4d5d-dfa6-82a5183b0f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------------+--------------------+-----------------+---------------------+-------------------------+-----------------------+--------------------+--------------+--------------------------+-----+-------------+---------+----+---------+---------+---------+---------+----+----+----+----+\n",
            "|       id|rodzaj_produkcji|               tytul|czy_dla_doroslych|rok_wydania_produkcji|rok_zakonczenia_produkcji|dlugosc_produkcji_w_min|             gatunek|liczba_sezonow|liczba_wszystkich_odcinkow|ocena|liczba_glosow|        1|  10|        2|        3|        4|        5|   6|   7|   8|   9|\n",
            "+---------+----------------+--------------------+-----------------+---------------------+-------------------------+-----------------------+--------------------+--------------+--------------------------+-----+-------------+---------+----+---------+---------+---------+---------+----+----+----+----+\n",
            "|tt0000002|           short|Le clown et ses c...|                0|                 1892|                       \\N|                      5|     Animation,Short|          null|                      null|  5.8|          261|nm0721526|null|nm1335271|     null|     null|     null|null|null|null|null|\n",
            "|tt0000004|           short|         Un bon bock|                0|                 1892|                       \\N|                     12|     Animation,Short|          null|                      null|  5.6|          176|nm0721526|null|nm1335271|     null|     null|     null|null|null|null|null|\n",
            "|tt0000008|           short|Edison Kinetoscop...|                0|                 1894|                       \\N|                      1|   Documentary,Short|          null|                      null|  5.4|         2068|nm0653028|null|nm0005690|nm0374658|     null|     null|null|null|null|null|\n",
            "|tt0000010|           short| Leaving the Factory|                0|                 1895|                       \\N|                      1|   Documentary,Short|          null|                      null|  6.9|         6990|nm0525910|null|     null|     null|     null|     null|null|null|null|null|\n",
            "|tt0000003|           short|      Pauvre Pierrot|                0|                 1892|                       \\N|                      4|Animation,Comedy,...|          null|                      null|  6.5|         1740|nm0721526|null|nm1770680|nm1335271|nm5442200|     null|null|null|null|null|\n",
            "|tt0000005|           short|    Blacksmith Scene|                0|                 1893|                       \\N|                      1|        Comedy,Short|          null|                      null|  6.2|         2553|nm0443482|null|nm0653042|nm0005690|nm0249379|     null|null|null|null|null|\n",
            "|tt0000001|           short|          Carmencita|                0|                 1894|                       \\N|                      1|   Documentary,Short|          null|                      null|  5.7|         1923|nm1588970|null|nm0005690|nm0374658|     null|     null|null|null|null|null|\n",
            "|tt0000006|           short|   Chinese Opium Den|                0|                 1894|                       \\N|                      1|               Short|          null|                      null|  5.1|          175|nm0005690|null|     null|     null|     null|     null|null|null|null|null|\n",
            "|tt0000009|           movie|          Miss Jerry|                0|                 1894|                       \\N|                     45|             Romance|          null|                      null|  5.3|          200|nm0063086|null|nm0183823|nm1309758|nm0085156|     null|null|null|null|null|\n",
            "|tt0000007|           short|Corbett and Court...|                0|                 1894|                       \\N|                      1|         Short,Sport|          null|                      null|  5.4|          797|nm0179163|null|nm0183947|nm0005690|nm0374658|nm0249379|null|null|null|null|\n",
            "+---------+----------------+--------------------+-----------------+---------------------+-------------------------+-----------------------+--------------------+--------------+--------------------------+-----+-------------+---------+----+---------+---------+---------+---------+----+----+----+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQetB-6I6Wb7"
      },
      "source": [
        "# Funkcja dodająca kolumnę z epokami"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "def df_column_string_to_int(df: DataFrame, column_name: str) -> DataFrame:\n",
        "  df_int_column = df.withColumn(column_name,col(column_name).cast(\"int\"))\n",
        "  return df_int_column"
      ],
      "metadata": {
        "id": "B1tKJ1oUMjOb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jiYcMc8HJqxL"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql import DataFrame\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "def add_epoch_column(df: DataFrame, periods = [1901,1918,1926,1939,1954,1970,1985,1994,2009]) -> DataFrame:\n",
        "\n",
        "  assert df.filter(df.rok_wydania_produkcji == \"\\\\N\").count() == 0\n",
        "  assert dict(df.dtypes)[\"rok_wydania_produkcji\"] == \"int\"\n",
        " \n",
        "  formulas = [ \n",
        "    when(col('rok_wydania_produkcji') <= period, str(index+1))  \n",
        "    for period, index \n",
        "    in zip(periods, range(len(periods)))]\n",
        "\n",
        "\n",
        "  df_periods = df.withColumn('epoka',\n",
        "    (reduce(lambda x,y: x.y, formulas)).otherwise( len(periods) +1)\n",
        "  )  \n",
        "\n",
        "  return df_periods"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "nwzcR281N7w8",
        "outputId": "b9430515-bc80-458e-e4d9-8e67cf05e32d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1a6ce2362cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = df_column_string_to_int(df.filter(df.id == 'tt0111161'),\"rok_wydania_produkcji\")\n",
        "sample_2 = df.filter(df.id == \"tt0080155\")\n",
        "\n",
        "add_epoch_column(sample)\n",
        "add_epoch_column(sample_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Nv_ZIt8ZF_3h",
        "outputId": "7c1c90a6-f994-4328-9888-4a8b2e488e30"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-65794920da94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tt0080155\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0madd_epoch_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0madd_epoch_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-831bb79a97c3>\u001b[0m in \u001b[0;36madd_epoch_column\u001b[0;34m(df, periods)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   df_periods = df.withColumn('epoka',\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformulas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperiods\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   )  \n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36motherwise\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \"\"\"\n\u001b[1;32m    866\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: otherwise() can only be applied on a Column previously generated by when()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "g6cuv_ChQpHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "import pytest\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark_test import assert_pyspark_df_equal\n",
        "from lib.feature_creators import add_epoch_column\n",
        "\n",
        "def test_add_epoch_column():\n",
        "\n",
        "  spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "  df = spark.createDataFrame(\n",
        "      [\n",
        "          (1, 1900),\n",
        "          (2, 1917),\n",
        "          (3, 1920),\n",
        "          (4, 1938),\n",
        "          (5, 1953),\n",
        "          (6, 1960),\n",
        "          (7, 1984),\n",
        "          (8, 1993),\n",
        "          (9, 2008),\n",
        "          (10, 2022)\n",
        "      ],  \n",
        "      \"id int, rok_wydania_produkcji int\",\n",
        "  )\n",
        "\n",
        "  expect_df = spark.createDataFrame(\n",
        "      [\n",
        "          (1, 1900, \"1\"),\n",
        "          (2, 1917, \"2\"),\n",
        "          (3, 1920, \"3\"),\n",
        "          (4, 1938, \"4\"),\n",
        "          (5, 1953, \"5\"),\n",
        "          (6, 1960, \"6\"),\n",
        "          (7, 1984, \"7\"),\n",
        "          (8, 1993, \"8\"),\n",
        "          (9, 2008, \"9\"),\n",
        "          (10, 2022, \"10\")\n",
        "      ],  \n",
        "      \"id int, rok_wydania_produkcji int, epoka string\",\n",
        "  )\n",
        "\n",
        "  result = add_epoch_column(df)\n",
        "  assert_pyspark_df_equal(result, expect_df)"
      ],
      "metadata": {
        "id": "_I2dbvayQpZh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "d601f96b-a93f-4973-de13-8ac2c28086f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6fbd35f2bbec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_add_epoch_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-197c10a0495b>\u001b[0m in \u001b[0;36mtest_add_epoch_column\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m   )\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_epoch_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0massert_pyspark_df_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-a818ab14b25c>\u001b[0m in \u001b[0;36madd_epoch_column\u001b[0;34m(df, periods)\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mformulas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rok_wydania_produkcji\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mperiods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperiods\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   df_periods = df.withColumn('epoka',\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformulas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   )  \n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Can't extract value from CASE WHEN (rok_wydania_produkcji#536 <= 1901) THEN 1 END: need struct type but got string"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1A73khuCZUaN4o2bn2e0rpNCDHiz6pWNS",
      "authorship_tag": "ABX9TyPqg2qfTVHk3A1Z42xWk7E3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}