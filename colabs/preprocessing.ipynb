{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Agowe5uPVKS"
   },
   "source": [
    "# Spark Setup and Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xea8rmW1Mtc8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0a7f8cb9-e1fc-4392-f6a9-a4a2b5eb0e63"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8My8u33PYJt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9ae83963-7a98-4a72-de0c-9fbfdf3ff283"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'BigData2022-films'...\n",
      "remote: Enumerating objects: 812, done.\u001B[K\n",
      "remote: Counting objects: 100% (424/424), done.\u001B[K\n",
      "remote: Compressing objects: 100% (244/244), done.\u001B[K\n",
      "remote: Total 812 (delta 274), reused 246 (delta 179), pack-reused 388\u001B[K\n",
      "Receiving objects: 100% (812/812), 2.91 MiB | 7.57 MiB/s, done.\n",
      "Resolving deltas: 100% (453/453), done.\n",
      "mv: cannot move 'BigData2022-films/.' to './.': Device or resource busy\n",
      "mv: cannot move 'BigData2022-films/..' to './..': Device or resource busy\n"
     ]
    }
   ],
   "source": [
    "# install Java8\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# download spark2.4.5\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "# unzip it\n",
    "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "# install findspark\n",
    "!pip install -q findspark\n",
    "# clone github repo\n",
    "!git clone https://github.com/PiotrMaciejKowalski/BigData2022-films\n",
    "# Przeniesienie plików z BigData2022-films do katalogu nadrzędnego\n",
    "!mv BigData2022-films/* .\n",
    "!mv BigData2022-films/.* .\n",
    "!rmdir BigData2022-films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fG3Xpjc4MvYY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# setup environment variables for our Spark Session to work\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = '/content/spark-3.2.1-bin-hadoop3.2'\n",
    "\n",
    "from lib.pyspark_startup import init, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIJChpFeBN-U"
   },
   "outputs": [],
   "source": [
    "spark = init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOLm43gvBWNM"
   },
   "outputs": [],
   "source": [
    "# Ładowanie danych z dysku google\n",
    "path = \"/content/drive/.shortcut-targets-by-id/1VcOir9FMG8LzEsUE-Q8YA79c_sV0tJwp/bigdata2022/\"\n",
    "\n",
    "df = spark.read.parquet(path + \"clean_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing - przyłożenie stworzonych funkcji"
   ],
   "metadata": {
    "id": "yXU7xq4qUiNk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from lib.pyspark_preprocesing import one_hot_encoding\n",
    "\n",
    "df = one_hot_encoding(df, [\"rodzaj_produkcji\",\"gatunek\"])"
   ],
   "metadata": {
    "id": "3I-HEm45a0wo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from lib.feature_creators import add_epoch_column\n",
    "\n",
    "df = add_epoch_column(df)"
   ],
   "metadata": {
    "id": "Fd-TvW73a_9U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from lib.film_people_list import people_film_merge_columns\n",
    "\n",
    "df = people_film_merge_columns(df,df['id'], add_column = True)"
   ],
   "metadata": {
    "id": "U_ebpfN6W4RN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Podział na zbiór uczący, walidacyjny i testowy"
   ],
   "metadata": {
    "id": "Vl0Y0n5XTPDx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train, valid, test = df.randomSplit([0.70, 0.2, 0.1], seed=123)"
   ],
   "metadata": {
    "id": "9lZQjmV_TOgj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3naR3zhLJDPH"
   },
   "source": [
    "# Zapis na dysku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hL93JLOGJF0Y"
   },
   "outputs": [],
   "source": [
    "train.write.mode(\"overwrite\").parquet(path + \"train_df.parquet\")\n",
    "valid.write.mode(\"overwrite\").parquet(path + \"valid_df.parquet\")\n",
    "test.write.mode(\"overwrite\").parquet(path + \"test_df.parquet\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.6 ('bigdata2022_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct 24 2022, 16:02:16) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d3bd7558b382e9dfedbadb497b519832d5eeed5cdea53be5faf7c2ce6e68cd89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}