{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Agowe5uPVKS"
      },
      "source": [
        "# Spark Setup and Data Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqkAJzxlWyMS"
      },
      "source": [
        "##Installation of Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8My8u33PYJt",
        "outputId": "dfdedf02-3ed7-408b-f685-d31c772c5757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default-java\t\t   java-11-openjdk-amd64     java-8-openjdk-amd64\n",
            "java-1.11.0-openjdk-amd64  java-1.8.0-openjdk-amd64\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (9.0.0)\n",
            "Collecting pyarrow\n",
            "  Downloading pyarrow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.0 MB 89 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow) (1.21.6)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 9.0.0\n",
            "    Uninstalling pyarrow-9.0.0:\n",
            "      Successfully uninstalled pyarrow-9.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 10.0.1 which is incompatible.\n",
            "db-dtypes 1.0.4 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 10.0.1 which is incompatible.\u001b[0m\n",
            "Successfully installed pyarrow-10.0.1\n",
            "Cloning into 'BigData2022-films'...\n",
            "remote: Enumerating objects: 346, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 346 (delta 112), reused 91 (delta 65), pack-reused 173\u001b[K\n",
            "Receiving objects: 100% (346/346), 2.49 MiB | 7.36 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n",
            "mv: cannot move 'BigData2022-films/.' to './.': Device or resource busy\n",
            "mv: cannot move 'BigData2022-films/..' to './..': Device or resource busy\n"
          ]
        }
      ],
      "source": [
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark2.4.5\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "# unzip it\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "# install findspark\n",
        "!pip install -q findspark\n",
        "# Google Colab has Java 11 available, test it using below command -\n",
        "!ls /usr/lib/jvm\n",
        "#install pyarrow\n",
        "!pip install -U pyarrow\n",
        "# clone github repo\n",
        "!git clone https://github.com/PiotrMaciejKowalski/BigData2022-films\n",
        "# Przeniesienie plików z BigData2022-films do katalogu nadrzędnego\n",
        "!mv BigData2022-films/* .\n",
        "!mv BigData2022-films/.* .\n",
        "!rmdir BigData2022-films"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "id": "RsD8qtgpCkhU",
        "outputId": "fa7610c4-7976-4642-affa-e8fe1c4dcfdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 227156\n",
            "drwxr-xr-x  1 root root      4096 Dec  6 20:05 .\n",
            "drwxr-xr-x  1 root root      4096 Dec  6 20:03 ..\n",
            "-rw-r--r--  1 root root        44 Dec  6 20:05 CODEOWNERS\n",
            "drwxr-xr-x  2 root root      4096 Dec  6 20:05 colabs\n",
            "drwxr-xr-x  4 root root      4096 Dec  5 14:36 .config\n",
            "drwxr-xr-x  2 root root      4096 Dec  6 20:05 docs\n",
            "drwxr-xr-x  8 root root      4096 Dec  6 20:05 .git\n",
            "-rw-r--r--  1 root root        62 Dec  6 20:05 .gitignore\n",
            "drwxr-xr-x  2 root root      4096 Dec  6 20:05 lib\n",
            "drwxr-xr-x  2 root root      4096 Dec  6 20:05 notebooks\n",
            "-rw-r--r--  1 root root       328 Dec  6 20:05 README.md\n",
            "drwxr-xr-x  2 root root      4096 Dec  6 20:05 reports\n",
            "-rw-r--r--  1 root root        26 Dec  6 20:05 requirements.txt\n",
            "drwxr-xr-x  1 root root      4096 Dec  5 14:37 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Feb  2  2020 spark-2.4.5-bin-hadoop2.7\n",
            "-rw-r--r--  1 root root 232530699 Feb  2  2020 spark-2.4.5-bin-hadoop2.7.tgz\n",
            "drwxr-xr-x  3 root root      4096 Dec  6 20:05 stripped\n",
            "drwxr-xr-x  2 root root      4096 Dec  6 20:05 tests\n",
            "drwxr-xr-x  3 root root      4096 Dec  6 20:05 tutorials\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git checkout ..."
      ],
      "metadata": {
        "id": "re9CJQ-AB4U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "zyk0FpbNCCwD",
        "outputId": "a87e078d-bb79-4a4f-d92a-d883768ff6a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\t\u001b[31mspark-2.4.5-bin-hadoop2.7.tgz\u001b[m\n",
            "\t\u001b[31mspark-2.4.5-bin-hadoop2.7/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fG3Xpjc4MvYY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "# setup environment variables for our Spark Session to work\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "from lib.pyspark_startup import init, load"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = init()"
      ],
      "metadata": {
        "id": "lIJChpFeBN-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x data_source.sh\n",
        "!./data_source.sh"
      ],
      "metadata": {
        "id": "xvL69RsNBd5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load(spark)"
      ],
      "metadata": {
        "id": "fOLm43gvBWNM",
        "outputId": "9c561cbb-ffb5-4c2b-9be9-4016250eaf0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1e7cea9c0e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "id": "9fvszGk1BxP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('bigdata2022_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "d3bd7558b382e9dfedbadb497b519832d5eeed5cdea53be5faf7c2ce6e68cd89"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}